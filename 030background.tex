Compared to the traditional slot-filling based dialog  \cite{williams2007partially,wen2017network,williams2017hybrid}, 
end-to-end training methods (e.g., \cite{BordesW16}, this work) do not require handcrafted state representations and their corresponding annotations in each dialog. Thus, they can easily be adapted to a new domain.  We discuss end-to-end approaches along two verticals: 1) decoder: whether the response is retrieved or generated and 2) encoder: how the dialog history and KB tuples are encoded.

Most of the existing end-to-end approaches  {\em retrieve} a response from a pre-defined set \cite{BordesW16,liu2017gated,seo2016query}. These methods are generally successful when they have to provide boilerplate responses -- they cannot construct responses by using words in KB not seen during training. 
Alternatively, generative approaches are used where the response is {\em generated} one word at a time \cite{eric2017copy,mem2seq}. These approaches mitigate the unseen entity problem by incorporating the ability to copy words from the input \cite{vinyals2015pointer,gu2016incorporating}. The copy mechanism has also found success in summarization \cite{nallapati2016abstractive,see2017get} and machine translation \cite{ptr-unk}. \sys\ is also a copy incorporated generative approach. 

For encoding, some approaches represent the dialog history as a sequence \cite{eric2017copy,ptr-unk}. Unfortunately, using a single long sequence for encoding also enforces an order over the set of KB tuples making it harder to perform inferencing over them. Other approaches represent the dialog context as a bag. Original Memory Networks \cite{BordesW16} and its extensions encode each memory element (utterance) as an average of all constituent words -- this cannot point to individual words, and hence cannot be used with a copy mechanism. Mem2Seq encodes each word individually in a flat memory. Unfortunately, this loses the contextual information around a word, which is needed to decipher an unseen word. In contrast, \sys\ uses a bag of sequences encoding, where KB tuples are a set for easier inference, and also each utterance is a sequence for effectively learning when to copy.  

